{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"cuInogGnHQ6Y","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{"id":"ez_KYPXiHQ6a"},"source":["# 1. Data Processing\n","\n","The majority of the code follows the provided template code in the [Competition's Train Demo Notebook](https://www.kaggle.com/code/smeitoma/train-demo/notebook), but this notebook uses an **LSTM model** to make stock price predictions instead of the Demo's LGBM model.\n","\n","The following functions are used to adjust the close prices in the raw stock price data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ednjMzZVHQ6c","trusted":true},"outputs":[],"source":["from decimal import ROUND_HALF_UP, Decimal\n","\n","def adjust_price(price):\n","    \"\"\"\n","    Args:\n","        price (pd.DataFrame)  : pd.DataFrame include stock_price\n","    Returns:\n","        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n","    \"\"\"\n","    # transform Date column into datetime\n","    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n","\n","    def generate_adjusted_close(df):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n","        Returns:\n","            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n","        \"\"\"\n","        # sort data to generate CumulativeAdjustmentFactor\n","        df = df.sort_values(\"Date\", ascending=False)\n","        # generate CumulativeAdjustmentFactor\n","        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n","        # generate AdjustedClose\n","        df.loc[:, \"AdjustedClose\"] = (\n","            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n","        ).map(lambda x: float(\n","            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n","        ))\n","        # reverse order\n","        df = df.sort_values(\"Date\")\n","        # to fill AdjustedClose, replace 0 into np.nan\n","        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n","        # forward fill AdjustedClose\n","        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n","        return df\n","\n","    # generate AdjustedClose\n","    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n","    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n","\n","    price.set_index(\"Date\", inplace=True)\n","    return price"]},{"cell_type":"markdown","metadata":{"id":"TOn6qyhiHQ6c"},"source":["We import the code necessary for the LSTM Model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zZUlf4HHQ6c","trusted":true},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import LSTM,Dense,Dropout\n","from keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{"id":"_iLuNaoJHQ6d"},"source":["We load the data, adjust closing prices, and view the resulting data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5N3IO2pHQ6d","trusted":true},"outputs":[],"source":["# Load stock price data\n","df_price = pd.read_csv(\"/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n","\n","# df_supp =  pd.read_csv(\"/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zH-hnAVHQ6d","outputId":"470fcb8f-4372-45ee-e746-daa311d23ca8","trusted":true},"outputs":[],"source":["df_price = adjust_price(df_price)\n","# df_supp = adjust_price(df_supp)\n","df_price.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"id":"pG-oqTgfHQ6d","outputId":"b5ecc328-1158-498c-e5ac-cf1354a94f19","trusted":true},"outputs":[],"source":["display(df_price.head(2))\n","display(df_price.tail(2))"]},{"cell_type":"markdown","metadata":{"id":"S0QmDSvaHQ6e"},"source":["The following function represents **Feature Engineering**, and we can use this to set features for the stock price data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6uTBNiEHQ6e","trusted":true},"outputs":[],"source":["def get_features_for_predict(price, code):\n","    \"\"\"\n","    Args:\n","        price (pd.DataFrame)  : pd.DataFrame include stock_price\n","        code (int)  : A local code for a listed company\n","    Returns:\n","        feature DataFrame (pd.DataFrame)\n","    \"\"\"\n","    close_col = \"AdjustedClose\"\n","    feats = price.loc[price[\"SecuritiesCode\"] == code, [\"SecuritiesCode\",\n","      close_col, \"ExpectedDividend\", \"High\", \"Low\", \"Open\", \"Close\"]].copy()\n","\n","    # single case\n","    feats[\"return_1day\"] = feats[close_col].pct_change(1)\n","\n","    # Amplitude\n","    feats[\"Amplitude\"] = feats[\"High\"] - feats[\"Low\"]\n","\n","    # Open to Close\n","    feats[\"OpentoClose\"] = feats[\"Open\"] - feats[\"Close\"]\n","\n","    # 52 Week High\n","    High52 = feats['AdjustedClose']/feats['High'].rolling(250).max()\n","    High52.rename('High52',inplace = True)\n","    feats = feats.merge(High52,left_index = True,right_index = True, how = 'left')\n","\n","    # MACD\n","    feats[\"MACD\"] = feats[close_col].ewm(span=12, adjust=False).mean() - feats[close_col].ewm(span=26, adjust=False).mean()\n","\n","    for period in [5, 10, 20, 40, 60]:\n","\n","      # calculate return using AdjustedClose\n","      feats[\"return_{}day\".format(period)] = feats[close_col].pct_change(period)\n","\n","      # volatility\n","      feats[\"volatility_{}day\".format(period)] = np.log(feats[close_col]).diff().rolling(period).std()\n","\n","      # moving average\n","      feats[\"MA_{}day\".format(period)] = feats[close_col].rolling(period).mean()\n","\n","      # exponential moving average\n","      feats[\"EMA_{}day\".format(period)] = feats[close_col].ewm(span=period, adjust=False).mean()\n","\n","      # ExpectedDividend\n","      feats[\"ExpectedDividend_{}\".format(period)] = feats[\"ExpectedDividend\"].mask(feats[\"ExpectedDividend\"] / feats[close_col] > period / 500, 1)\n","\n","      # RSI\n","      C_Diff = feats['AdjustedClose'] - feats['AdjustedClose'].shift(1)\n","      U = C_Diff.apply(lambda series: series if series > 0 else 0)\n","      D = C_Diff.apply(lambda series: -series if series < 0 else 0)\n","      EMA_U = U.ewm(span = period, adjust = False).mean()\n","      EMA_D = D.ewm(span = period, adjust = False).mean()\n","      RSI = EMA_U/(EMA_U+EMA_D) * 100\n","      RSI.rename('RSI_{}day'.format(period),inplace = True)\n","      feats = feats.merge(RSI,left_index = True,right_index = True,how = 'left')\n","\n","      # MACD\n","      feats[\"MACD_{}day\".format(period)] = feats[close_col].ewm(span=period,\n","        adjust=False).mean() - feats[close_col].ewm(span=2*period, adjust=False).mean()\n","\n","      # BIAS\n","      BIAS = feats['AdjustedClose'].rolling(period).mean()\n","      BIAS = (feats['AdjustedClose'] - BIAS)/BIAS\n","      BIAS.rename('BIAS_{}day'.format(period),inplace = True)\n","      feats = feats.merge(BIAS,left_index = True,right_index = True, how = 'left')\n","\n","    # filling data for nan and inf\n","    feats = feats.fillna(0)\n","    feats = feats.replace([np.inf, -np.inf], 0)\n","    # drop AdjustedClose column\n","    feats = feats.drop([close_col], axis=1)\n","\n","    return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0bcfXmTHQ6e","outputId":"3a3f90a0-4442-40fc-cfe9-130fdb78099e","trusted":true},"outputs":[],"source":["# fetch prediction target SecuritiesCodes\n","# There are 2000 codes\n","codes = sorted(df_price[\"SecuritiesCode\"].unique())\n","len(codes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N--JUuF0HQ6e","outputId":"fd79c4f4-628e-4a7c-b286-c94efe39a573","trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","# generate the features for prediction\n","buff = []\n","for code in tqdm(codes):\n","    feat = get_features_for_predict(df_price, code)\n","    buff.append(feat)\n","feature = pd.concat(buff)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"id":"Ie2qeTvjHQ6f","outputId":"435cc02e-773b-42df-c909-e64fc6366223","trusted":true},"outputs":[],"source":["display(feature.head(2))\n","display(feature.tail(2))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ByCGL5hb_F5H","outputId":"0039b06a-22e7-46d0-fa05-860adf469c70","trusted":true},"outputs":[],"source":["feature.info()"]},{"cell_type":"markdown","metadata":{},"source":["Execute PCA on the data with full features and retain only the necessary components as features."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","data_key_features = feature.copy()\n","data_codes = data_key_features[[\"SecuritiesCode\"]]\n","\n","pca = PCA(n_components = 'mle')\n","data_components = pca.fit_transform(feature)\n","\n","data_components = pd.DataFrame(data_components)\n","data_components[\"SecuritiesCode\"] = data_codes.values\n","data_components[\"Date\"] = feature.index.values\n","data_components.set_index(\"Date\", inplace=True)\n","\n","sum(pca.explained_variance_ratio_[:2]) # >95% of the variance\n","\n","data_components = data_components[[\"SecuritiesCode\", 0, 1, 2]] # first 3 components"]},{"cell_type":"markdown","metadata":{},"source":["We perform feature subset selection."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_label(price, code):\n","    \"\"\" Labelizer\n","    Args:\n","        price (pd.DataFrame): dataframe of stock_price.csv\n","        code (int): Local Code in the universe\n","    Returns:\n","        df (pd.DataFrame): label data\n","    \"\"\"\n","    df = price.loc[price[\"SecuritiesCode\"] == code].copy()\n","    df.loc[:, \"label\"] = df[\"Target\"]\n","\n","    return df.loc[:, [\"SecuritiesCode\", \"label\"]]\n","\n","# split data into TRAIN and TEST\n","TRAIN_END = \"2019-12-31\"\n","# We put a week gap between TRAIN_END and TEST_START\n","# to avoid leakage of test data information from label\n","TEST_START = \"2020-01-06\"\n","\n","\n","def get_features_and_label(price, codes, features):\n","    \"\"\"\n","    Args:\n","        price (pd.DataFrame): loaded price data\n","        codes  (array) : target codes\n","        feature (pd.DataFrame): features\n","    Returns:\n","        train_X (pd.DataFrame): training data\n","        train_y (pd.DataFrame): label for train_X\n","        test_X (pd.DataFrame): test data\n","        test_y (pd.DataFrame): label for test_X\n","    \"\"\"\n","    # to store splited data\n","    trains_X, tests_X = [], []\n","    trains_y, tests_y = [], []\n","\n","    # generate feature one by one\n","    for code in tqdm(codes):\n","\n","        feats = features[features[\"SecuritiesCode\"] == code].dropna()\n","        labels = get_label(price, code).dropna()\n","        \n","        if feats.shape[0] > 0 and labels.shape[0] > 0:\n","            # align label and feature indexes\n","            labels = labels.loc[labels.index.isin(feats.index)]\n","            feats = feats.loc[feats.index.isin(labels.index)]\n","            \n","            assert (labels.loc[:, \"SecuritiesCode\"] == feats.loc[:, \"SecuritiesCode\"]).all()\n","            labels = labels.loc[:, \"label\"]\n","\n","            # split data into TRAIN and TEST\n","            _train_X = feats[: TRAIN_END]\n","            _test_X = feats[TEST_START:]\n","\n","            _train_y = labels[: TRAIN_END]\n","            _test_y = labels[TEST_START:]\n","            \n","            assert len(_train_X) == len(_train_y)\n","            assert len(_test_X) == len(_test_y)\n","\n","            # store features\n","            trains_X.append(_train_X)\n","            tests_X.append(_test_X)\n","            # store labels\n","            trains_y.append(_train_y)\n","            tests_y.append(_test_y)\n","            \n","    # combine features for each codes\n","    train_X = pd.concat(trains_X)\n","    test_X = pd.concat(tests_X)\n","    # combine label for each codes\n","    train_y = pd.concat(trains_y)\n","    test_y = pd.concat(tests_y)\n","\n","    return train_X, train_y, test_X, test_y"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !Use this cell for PCA case only!:\n","\n","train_X, train_y, test_X, test_y = get_features_and_label(\n","    df_price, codes, data_components\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !Use this cell for no PCA case:\n","\n","train_X, train_y, test_X, test_y = get_features_and_label(\n","    df_price, codes, feature\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","train_X.loc[:, 'Target'] = train_y\n","test_X.loc[:, 'Target'] = test_y\n","train_X.reset_index(inplace = True)\n","test_X.reset_index(inplace = True)\n","df_price.reset_index(inplace = True)\n","train_X = df_price[[\"Date\", \"SecuritiesCode\"]].merge(train_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\n","test_X = df_price[[\"Date\", \"SecuritiesCode\"]].merge(test_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\n","train_X.dropna(inplace = True)\n","test_X.dropna(inplace = True)\n","\n","train_y = train_X['Target']\n","test_y = test_X['Target']\n","train_X.drop(\"Target\", axis = 1, inplace = True);\n","test_X.drop(\"Target\", axis = 1, inplace = True);\n","\n","print(\"train_X has shape\", train_X.shape)\n","print(\"train_y has shape\", train_y.shape)\n","print(\"test_X has shape\", test_X.shape)\n","print(\"test_y has shape\", test_y.shape)\n","\n","train_X.set_index('Date', inplace = True)\n","test_X.set_index('Date', inplace = True)\n","\n","old_test_X = test_X\n","old_train_X = train_X\n","# feat_cols = list(range(1, 4))\n","# train_X = train_X.iloc[:, feat_cols]\n","# test_X = test_X.iloc[:, feat_cols]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_X = old_test_X\n","train_X = old_train_X\n","\n","print(\"train_X has shape\", train_X.shape)\n","print(\"train_y has shape\", train_y.shape)\n","print(\"test_X has shape\", test_X.shape)\n","print(\"test_y has shape\", test_y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feats_list = train_X.columns.to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !!! ONLY Use this cell for no PCA case !!!\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers import LSTM,Dense,Dropout\n","from keras.optimizers import Adam\n","from sklearn.metrics import mean_squared_error\n","\n","results_lstm = []\n","\n","train_X_lstm = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n","test_X_lstm = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))\n","\n","model = Sequential()\n","model.add(LSTM(units = 64,input_shape = (7, 1)))\n","model.add(Dropout(0.4))\n","model.add(Dense(1))\n","model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n","model.fit(train_X_lstm[:, :, :],train_y,batch_size = 4096,epochs = 10)\n","LSTM_test_pred = model.predict(test_X_lstm[:, :, :])\n","result = old_test_X[[\"SecuritiesCode\"]].copy()\n","result.loc[:, \"Predict\"] = LSTM_test_pred\n","result.loc[:, 'Target'] = test_y.values\n","\n","results_lstm.append(np.sqrt(mean_squared_error(test_y, LSTM_test_pred)))\n","\n","result = result.sort_values([\"Date\", \"Predict\"], ascending=[True, False])\n","result = result.groupby(\"Date\").apply(set_rank)\n","\n","# add this subset's Sharpe Ratio to results\n","results_lstm.append(calc_spread_return_sharpe(result, portfolio_size=200))\n","    \n","old_train_X.iloc[:, feat_cols]\n","old_test_X.iloc[:, feat_cols]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(results_lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(train_X_lstm.shape)"]},{"cell_type":"markdown","metadata":{},"source":["We explore the feature weights found."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import shap\n","DE = shap.DeepExplainer(model, train_X_lstm)\n","shap_values = DE.shap_values(test_X_lstm, check_additivity=False) # X_validate is 3d numpy.ndarray\n","\n","shap.initjs()\n","shap.summary_plot(\n","    shap_values[0], \n","    test_X_lstm,\n","    feature_names=train_X.columns.tolist(),\n","    max_display=50,\n","    plot_type='bar')"]},{"cell_type":"markdown","metadata":{},"source":["Feature Subsets:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# feature_subsets = {\n","#     \"012\": {\"train_X\": train_X, \"test_X\": test_X},\n","#     \"01\": {\"train_X\": train_X[[\"SecuritiesCode\", 0, 1]], \"test_X\": test_X[[\"SecuritiesCode\", 0, 1]]},\n","#     \"02\": {\"train_X\": train_X[[\"SecuritiesCode\", 0, 2]], \"test_X\": test_X[[\"SecuritiesCode\", 0, 2]]},\n","#     \"12\": {\"train_X\": train_X[[\"SecuritiesCode\", 1, 2]], \"test_X\": test_X[[\"SecuritiesCode\", 1, 2]]},\n","#     \"0\": {\"train_X\": train_X[[\"SecuritiesCode\", 0]], \"test_X\": test_X[[\"SecuritiesCode\", 0]]},\n","#     \"1\": {\"train_X\": train_X[[\"SecuritiesCode\", 1]], \"test_X\": test_X[[\"SecuritiesCode\", 1]]},\n","#     \"2\": {\"train_X\": train_X[[\"SecuritiesCode\", 2]], \"test_X\": test_X[[\"SecuritiesCode\", 2]]}\n","# }\n","\n","feature_subsets = {\n","    \"012\": {\"train_X\": train_X, \"test_X\": test_X},\n","    \"01\": {\"train_X\": train_X[[0, 1]], \"test_X\": test_X[[0, 1]]},\n","    \"02\": {\"train_X\": train_X[[0, 2]], \"test_X\": test_X[[0, 2]]},\n","    \"12\": {\"train_X\": train_X[[1, 2]], \"test_X\": test_X[[1, 2]]},\n","    \"0\": {\"train_X\": train_X[[0]], \"test_X\": test_X[[0]]},\n","    \"1\": {\"train_X\": train_X[[1]], \"test_X\": test_X[[1]]},\n","    \"2\": {\"train_X\": train_X[[2]], \"test_X\": test_X[[2]]}\n","}\n","\n","feature_subsets_list = [\"012\", \"01\", \"02\", \"12\", \"0\", \"1\", \"2\"]"]},{"cell_type":"markdown","metadata":{},"source":["Let's see an example of how to access one subset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(feature_subsets[\"012\"][\"train_X\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train = feature_subsets[\"01\"][\"train_X\"].values\n","X_test = feature_subsets[\"01\"][\"test_X\"].values\n","y_train = train_y.values\n","y_test = test_y.values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"train_X has shape\", X_train.shape)\n","print(\"train_y has shape\", y_train.shape)\n","print(\"test_X has shape\", X_test.shape)\n","print(\"test_y has shape\", y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Training\n","Let's train Transformer, LSTM, XGBoost, and Lasso models on all the subsets:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training Code Starts"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def set_rank(df):\n","    \"\"\"\n","    Args:\n","        df (pd.DataFrame): including predict column\n","    Returns:\n","        df (pd.DataFrame): df with Rank\n","    \"\"\"\n","    # sort records to set Rank\n","    df = df.sort_values(\"Predict\", ascending=False)\n","    # set Rank starting from 0\n","    df.loc[:, \"Rank\"] = np.arange(len(df[\"Predict\"]))\n","    return df\n","\n","def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n","    \"\"\"\n","    Args:\n","        df (pd.DataFrame): predicted results\n","        portfolio_size (int): # of equities to buy/sell\n","        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n","    Returns:\n","        (float): sharpe ratio\n","    \"\"\"\n","    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n","        \"\"\"\n","        Args:\n","            df (pd.DataFrame): predicted results\n","            portfolio_size (int): # of equities to buy/sell\n","            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n","        Returns:\n","            (float): spread return\n","        \"\"\"\n","        assert df['Rank'].min() == 0\n","        assert df['Rank'].max() == len(df['Rank']) - 1\n","        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n","        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n","        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n","        return purchase - short\n","\n","    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n","    sharpe_ratio = buf.mean() / buf.std()\n","    return sharpe_ratio"]},{"cell_type":"markdown","metadata":{},"source":["Train an LSTM model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import LSTM,Dense,Dropout\n","from keras.optimizers import Adam\n","from sklearn.metrics import mean_squared_error\n","\n","results_lstm = []\n","\n","iter = 1\n","for subset_key in feature_subsets_list:\n","\n","    print(\"On iteration\", iter, \"of\", len(feature_subsets_list), \": Running subset with components (features)\", subset_key)\n","    \n","    train_X_lstm = np.reshape(feature_subsets[subset_key]['train_X'], (feature_subsets[subset_key]['train_X'].shape[0], feature_subsets[subset_key]['train_X'].shape[1], 1))\n","    test_X_lstm = np.reshape(feature_subsets[subset_key]['test_X'], (feature_subsets[subset_key]['test_X'].shape[0], feature_subsets[subset_key]['test_X'].shape[1], 1))\n","\n","    model = Sequential()\n","    model.add(LSTM(units = 64,input_shape = (7, 1)))\n","    model.add(Dropout(0.4))\n","    model.add(Dense(1))\n","    model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n","    model.fit(train_X_lstm[:, :, :],train_y,batch_size = 4096,epochs = 10)\n","    LSTM_test_pred = model.predict(test_X_lstm[:, :, :])\n","    result = old_test_X[[\"SecuritiesCode\"]].copy()\n","    result.loc[:, \"Predict\"] = LSTM_test_pred\n","    result.loc[:, 'Target'] = test_y.values\n","\n","    result = result.sort_values([\"Date\", \"Predict\"], ascending=[True, False])\n","    result = result.groupby(\"Date\").apply(set_rank)\n","    \n","    # add this subset's Sharpe Ratio to results\n","    results_lstm.append({subset_key: [calc_spread_return_sharpe(result, portfolio_size=200), np.sqrt(mean_squared_error(test_y, LSTM_test_pred))]})\n","    iter += 1\n","    \n","old_train_X.iloc[:, feat_cols]\n","old_test_X.iloc[:, feat_cols]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(results_lstm)"]},{"cell_type":"markdown","metadata":{},"source":["We learn that LSTM performs the best when only components 01 is used, with a Sharpe Ratio of 0.12!"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":3935619,"sourceId":34349,"sourceType":"competition"}],"dockerImageVersionId":30684,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
