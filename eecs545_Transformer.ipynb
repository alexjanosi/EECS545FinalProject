{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'jpx-tokyo-stock-exchange-prediction:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F34349%2F3935619%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240430%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240430T211348Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7a47daae27276d549c1f1df0cfeea0f135b5aff16583c390887b5bc146dcd2b15f228d8be8ba8e2a45704852f35e210efa09beb70f5f9c8fad0c5a11e9654c0a3338f4465679883fc18e1078f58fc9751a5f5de9b80901b107d3b7ec41716435cda4163e2d9fbd1476874ac88b46dc056b7fafd5f162dcc1b73393c2d8df25b8ff7dd008041316a8aac3c015fb0881275ecaaceda1b1920d0d798a5330da1acda0dedddda95476c371cd850d89a2eb7f510409c48383db1268385b2d440d6d9ee7efbf1fea6b8322e3c431fd8513bcd456a4443d22b1372f9c81ded0fa4f549da5c7fea8045dd822015be9c2abfb5237e4f4154e5c340585a9c2128a212bf2e5'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zsSsRcYfjFj",
        "outputId": "92f27b9e-52db-46db-bca5-a3721f8adbdd"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading jpx-tokyo-stock-exchange-prediction, 252341457 bytes compressed\n",
            "[==================================================] 252341457 bytes downloaded\n",
            "Downloaded and uncompressed: jpx-tokyo-stock-exchange-prediction\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "hohDMhgfkP_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "def seed_everything(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "nra87YQPR7m-",
        "execution": {
          "iopub.status.busy": "2022-06-05T04:13:07.238493Z",
          "iopub.execute_input": "2022-06-05T04:13:07.239163Z",
          "iopub.status.idle": "2022-06-05T04:13:20.763023Z",
          "shell.execute_reply.started": "2022-06-05T04:13:07.239057Z",
          "shell.execute_reply": "2022-06-05T04:13:20.762092Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config - GPU.\n",
        "Either GPU or TPU - Comment one of them"
      ],
      "metadata": {
        "id": "hkIWtAjLkOOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #GPU\n",
        "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# gpus = tf.config.list_physical_devices('GPU')\n",
        "# if gpus:\n",
        "#     try:\n",
        "#         # Currently, memory growth needs to be the same across GPUs\n",
        "#         for gpu in gpus:\n",
        "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#         logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "#     except RuntimeError as e:\n",
        "#         # Memory growth must be set before GPUs have been initialized\n",
        "#         print(e)\n",
        "# strategy = tf.distribute.MirroredStrategy()\n",
        "# print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "metadata": {
        "id": "Y-k6XC6tkG0b",
        "execution": {
          "iopub.status.busy": "2022-06-05T04:13:20.764949Z",
          "iopub.execute_input": "2022-06-05T04:13:20.765196Z",
          "iopub.status.idle": "2022-06-05T04:13:20.77054Z",
          "shell.execute_reply.started": "2022-06-05T04:13:20.765161Z",
          "shell.execute_reply": "2022-06-05T04:13:20.769637Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config - TPU.\n",
        "Either GPU or TPU - Comment one of them"
      ],
      "metadata": {
        "id": "rCnlIE9LQSV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TPU\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.TPUStrategy(tpu)\n",
        "print('Running on TPU ', tpu.master())\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "id": "toXbGXqQQOnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83b5c72-1639-42a8-ce5e-c44fea83df54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  \n",
            "REPLICAS:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "5Iyrx8U-kSek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following functions are used to adjust the close prices in the raw stock price data.\n",
        "# We will generate AdjustedClose using AdjustmentFactor value. This should reduce historical price gap caused by split/reverse-split.\n",
        "\n",
        "from decimal import ROUND_HALF_UP, Decimal\n",
        "\n",
        "def adjust_price(price):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        price (pd.DataFrame)  : pd.DataFrame include stock_price\n",
        "    Returns:\n",
        "        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n",
        "    \"\"\"\n",
        "    # transform Date column into datetime\n",
        "    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n",
        "\n",
        "    def generate_adjusted_close(df):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n",
        "        Returns:\n",
        "            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n",
        "        \"\"\"\n",
        "        # sort data to generate CumulativeAdjustmentFactor\n",
        "        df = df.sort_values(\"Date\", ascending=False)\n",
        "        # generate CumulativeAdjustmentFactor\n",
        "        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n",
        "        # generate AdjustedClose\n",
        "        df.loc[:, \"AdjustedClose\"] = (\n",
        "            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n",
        "        ).map(lambda x: float(\n",
        "            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n",
        "        ))\n",
        "        # reverse order\n",
        "        df = df.sort_values(\"Date\")\n",
        "        # to fill AdjustedClose, replace 0 into np.nan\n",
        "        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n",
        "        # forward fill AdjustedClose\n",
        "        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n",
        "        return df\n",
        "\n",
        "    # generate AdjustedClose\n",
        "    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n",
        "    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n",
        "\n",
        "    price.set_index(\"Date\", inplace=True)\n",
        "    return price"
      ],
      "metadata": {
        "id": "l7cMLPDihZMZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_price_data = pd.read_csv(\"/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n",
        "stock_price_adj_data = adjust_price(stock_price_data)\n",
        "stock_price_adj_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e3Q9f_jhqD1",
        "outputId": "d7a06f45-4591-4409-9aa1-054a8d28446d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 2332531 entries, 2017-01-04 to 2021-12-03\n",
            "Data columns (total 13 columns):\n",
            " #   Column                      Dtype  \n",
            "---  ------                      -----  \n",
            " 0   RowId                       object \n",
            " 1   SecuritiesCode              int64  \n",
            " 2   Open                        float64\n",
            " 3   High                        float64\n",
            " 4   Low                         float64\n",
            " 5   Close                       float64\n",
            " 6   Volume                      int64  \n",
            " 7   AdjustmentFactor            float64\n",
            " 8   ExpectedDividend            float64\n",
            " 9   SupervisionFlag             bool   \n",
            " 10  Target                      float64\n",
            " 11  CumulativeAdjustmentFactor  float64\n",
            " 12  AdjustedClose               float64\n",
            "dtypes: bool(1), float64(9), int64(2), object(1)\n",
            "memory usage: 233.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_for_predict(price, code):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        price (pd.DataFrame)  : pd.DataFrame include stock_price\n",
        "        code (int)  : A local code for a listed company\n",
        "    Returns:\n",
        "        feature DataFrame (pd.DataFrame)\n",
        "    \"\"\"\n",
        "    close_col = \"AdjustedClose\"\n",
        "    feats = price.loc[price[\"SecuritiesCode\"] == code, [\"SecuritiesCode\",\n",
        "      close_col, \"ExpectedDividend\", \"High\", \"Low\", \"Open\", \"Close\"]].copy()\n",
        "\n",
        "    # single case\n",
        "    feats[\"return_1day\"] = feats[close_col].pct_change(1)\n",
        "\n",
        "    # ExpectedDividend\n",
        "    feats[\"ExpectedDividend\"] = feats[\"ExpectedDividend\"].mask(feats[\"ExpectedDividend\"] > 0, 1)\n",
        "\n",
        "    # Amplitude\n",
        "    feats[\"Amplitude\"] = feats[\"High\"] - feats[\"Low\"]\n",
        "\n",
        "    # Open to Close\n",
        "    feats[\"OpentoClose\"] = feats[\"Open\"] - feats[\"Close\"]\n",
        "\n",
        "    # 52 Week High\n",
        "    High52 = feats['AdjustedClose']/feats['High'].rolling(250).max()\n",
        "    High52.rename('High52',inplace = True)\n",
        "    feats = feats.merge(High52,left_index = True,right_index = True, how = 'left')\n",
        "\n",
        "    # MACD\n",
        "    feats[\"MACD\"] = feats[close_col].ewm(span=12, adjust=False).mean() - feats[close_col].ewm(span=26, adjust=False).mean()\n",
        "\n",
        "    for period in [5, 10, 20, 40, 60]:\n",
        "\n",
        "      # calculate return using AdjustedClose\n",
        "      feats[\"return_{}day\".format(period)] = feats[close_col].pct_change(period)\n",
        "\n",
        "      # volatility\n",
        "      feats[\"volatility_{}day\".format(period)] = np.log(feats[close_col]).diff().rolling(period).std()\n",
        "\n",
        "      # moving average\n",
        "      feats[\"MA_{}day\".format(period)] = feats[close_col].rolling(period).mean()\n",
        "\n",
        "      # exponential moving average\n",
        "      feats[\"EMA_{}day\".format(period)] = feats[close_col].ewm(span=period, adjust=False).mean()\n",
        "\n",
        "      # RSI\n",
        "      C_Diff = feats['AdjustedClose'] - feats['AdjustedClose'].shift(1)\n",
        "      U = C_Diff.apply(lambda series: series if series > 0 else 0)\n",
        "      D = C_Diff.apply(lambda series: -series if series < 0 else 0)\n",
        "      EMA_U = U.ewm(span = period, adjust = False).mean()\n",
        "      EMA_D = D.ewm(span = period, adjust = False).mean()\n",
        "      RSI = EMA_U/(EMA_U+EMA_D) * 100\n",
        "      RSI.rename('RSI_{}day'.format(period),inplace = True)\n",
        "      feats = feats.merge(RSI,left_index = True,right_index = True,how = 'left')\n",
        "\n",
        "      # MACD\n",
        "      feats[\"MACD_{}day\".format(period)] = feats[close_col].ewm(span=period,\n",
        "        adjust=False).mean() - feats[close_col].ewm(span=2*period, adjust=False).mean()\n",
        "\n",
        "      # BIAS\n",
        "      BIAS = feats['AdjustedClose'].rolling(period).mean()\n",
        "      BIAS = (feats['AdjustedClose'] - BIAS)/BIAS\n",
        "      BIAS.rename('BIAS_{}day'.format(period),inplace = True)\n",
        "      feats = feats.merge(BIAS,left_index = True,right_index = True, how = 'left')\n",
        "\n",
        "    # filling data for nan and inf\n",
        "    feats = feats.fillna(0)\n",
        "    feats = feats.replace([np.inf, -np.inf], 0)\n",
        "    # drop AdjustedClose column\n",
        "    feats = feats.drop([close_col], axis=1)\n",
        "\n",
        "    return feats"
      ],
      "metadata": {
        "id": "P3vPwPwShxDh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch prediction target SecuritiesCodes\n",
        "# There are 2000 codes\n",
        "codes = sorted(stock_price_adj_data[\"SecuritiesCode\"].unique())\n",
        "len(codes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-e5h3lchxNx",
        "outputId": "d92c57d6-dcf1-44a1-effd-afd874a7e560"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Smart progress meter\n",
        "# generate the features for prediction\n",
        "buff = []\n",
        "for code in tqdm(codes):\n",
        "    feat = get_features_for_predict(stock_price_adj_data, code)\n",
        "    buff.append(feat)\n",
        "feature = pd.concat(buff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkkOULeBhxSS",
        "outputId": "b1038816-4555-46dd-b66a-aca696bc2486"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [01:38<00:00, 20.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature.info()"
      ],
      "metadata": {
        "id": "nz0MeWMHA7vP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1277481a-0758-4ef8-9db7-fa74073e1473"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 2332531 entries, 2017-01-04 to 2021-12-03\n",
            "Data columns (total 46 columns):\n",
            " #   Column            Dtype  \n",
            "---  ------            -----  \n",
            " 0   SecuritiesCode    int64  \n",
            " 1   ExpectedDividend  float64\n",
            " 2   High              float64\n",
            " 3   Low               float64\n",
            " 4   Open              float64\n",
            " 5   Close             float64\n",
            " 6   return_1day       float64\n",
            " 7   Amplitude         float64\n",
            " 8   OpentoClose       float64\n",
            " 9   High52            float64\n",
            " 10  MACD              float64\n",
            " 11  return_5day       float64\n",
            " 12  volatility_5day   float64\n",
            " 13  MA_5day           float64\n",
            " 14  EMA_5day          float64\n",
            " 15  RSI_5day          float64\n",
            " 16  MACD_5day         float64\n",
            " 17  BIAS_5day         float64\n",
            " 18  return_10day      float64\n",
            " 19  volatility_10day  float64\n",
            " 20  MA_10day          float64\n",
            " 21  EMA_10day         float64\n",
            " 22  RSI_10day         float64\n",
            " 23  MACD_10day        float64\n",
            " 24  BIAS_10day        float64\n",
            " 25  return_20day      float64\n",
            " 26  volatility_20day  float64\n",
            " 27  MA_20day          float64\n",
            " 28  EMA_20day         float64\n",
            " 29  RSI_20day         float64\n",
            " 30  MACD_20day        float64\n",
            " 31  BIAS_20day        float64\n",
            " 32  return_40day      float64\n",
            " 33  volatility_40day  float64\n",
            " 34  MA_40day          float64\n",
            " 35  EMA_40day         float64\n",
            " 36  RSI_40day         float64\n",
            " 37  MACD_40day        float64\n",
            " 38  BIAS_40day        float64\n",
            " 39  return_60day      float64\n",
            " 40  volatility_60day  float64\n",
            " 41  MA_60day          float64\n",
            " 42  EMA_60day         float64\n",
            " 43  RSI_60day         float64\n",
            " 44  MACD_60day        float64\n",
            " 45  BIAS_60day        float64\n",
            "dtypes: float64(45), int64(1)\n",
            "memory usage: 836.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label creation\n",
        "# Next, we obtain the labels to be used for training the model ...\n",
        "# ... (this is where we load and split the label data).\n",
        "\n",
        "def get_label(price, code):\n",
        "    \"\"\" Labelizer\n",
        "    Args:\n",
        "        price (pd.DataFrame): dataframe of stock_price.csv\n",
        "        code (int): Local Code in th/e universe\n",
        "    Returns:\n",
        "        df (pd.DataFrame): label data\n",
        "    \"\"\"\n",
        "    df = price.loc[price[\"SecuritiesCode\"] == code].copy()\n",
        "    df.loc[:, \"label\"] = df[\"Target\"]\n",
        "\n",
        "    return df.loc[:, [\"SecuritiesCode\", \"label\"]]"
      ],
      "metadata": {
        "id": "vbvb2eczh4k7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We split the data into **Train** and **Test** sets. ...\n",
        "# This can also be updated to obtain **Validation** sets later on.\n",
        "\n",
        "# split data into TRAIN and TEST\n",
        "TRAIN_END = \"2019-02-22\"\n",
        "VALID_START = \"2019-03-01\"\n",
        "VALID_END = \"2019-12-31\"\n",
        "# We put a week gap between TRAIN_END and TEST_START\n",
        "# to avoid leakage of test data information from label\n",
        "TEST_START = \"2020-01-06\"\n",
        "\n",
        "def get_features_and_label(price, codes, features):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        price (pd.DataFrame): loaded price data\n",
        "        codes  (array) : target codes\n",
        "        feature (pd.DataFrame): features\n",
        "    Returns:\n",
        "        train_X (pd.DataFrame): training data\n",
        "        train_y (pd.DataFrame): label for train_X\n",
        "        valid_X (pd.DataFrame): validation data\n",
        "        valid_y (pd.DataFrame): label for validation\n",
        "        test_X (pd.DataFrame): test data\n",
        "        test_y (pd.DataFrame): label for test_X\n",
        "    \"\"\"\n",
        "    # to store splited data\n",
        "    trains_X, valid_X, tests_X = [], [], []\n",
        "    trains_y, valid_y, tests_y = [], [], []\n",
        "\n",
        "    # generate feature one by one\n",
        "    for code in tqdm(codes):\n",
        "\n",
        "        feats = features[features[\"SecuritiesCode\"] == code].dropna()\n",
        "        labels = get_label(price, code).dropna()\n",
        "\n",
        "        if feats.shape[0] > 0 and labels.shape[0] > 0:\n",
        "            # align label and feature indexes\n",
        "            labels = labels.loc[labels.index.isin(feats.index)]\n",
        "            feats = feats.loc[feats.index.isin(labels.index)]\n",
        "\n",
        "            assert (labels.loc[:, \"SecuritiesCode\"] == feats.loc[:, \"SecuritiesCode\"]).all()\n",
        "            labels = labels.loc[:, \"label\"]\n",
        "\n",
        "            # split data into TRAIN and TEST\n",
        "            _train_X = feats[: TRAIN_END]\n",
        "            _valid_X = feats[VALID_START: VALID_END]\n",
        "            _test_X = feats[TEST_START:]\n",
        "\n",
        "            _train_y = labels[: TRAIN_END]\n",
        "            _valid_y = labels[VALID_START: VALID_END]\n",
        "            _test_y = labels[TEST_START:]\n",
        "\n",
        "            assert len(_train_X) == len(_train_y)\n",
        "            assert len(_valid_X) == len(_valid_y)\n",
        "            assert len(_test_X) == len(_test_y)\n",
        "\n",
        "            # store features\n",
        "            trains_X.append(_train_X)\n",
        "            valid_X.append(_valid_X)\n",
        "            tests_X.append(_test_X)\n",
        "            # store labels\n",
        "            trains_y.append(_train_y)\n",
        "            valid_y.append(_valid_y)\n",
        "            tests_y.append(_test_y)\n",
        "\n",
        "    # combine features for each codes\n",
        "    train_X = pd.concat(trains_X)\n",
        "    valid_X = pd.concat(valid_X)\n",
        "    test_X = pd.concat(tests_X)\n",
        "    # combine label for each codes\n",
        "    train_y = pd.concat(trains_y)\n",
        "    valid_y = pd.concat(valid_y)\n",
        "    test_y = pd.concat(tests_y)\n",
        "\n",
        "    return train_X, train_y, valid_X, valid_y, test_X, test_y"
      ],
      "metadata": {
        "id": "R99VEMZVh5MW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate feature/label\n",
        "train_X, train_y, valid_X, valid_y, test_X, test_y = get_features_and_label(\n",
        "    stock_price_adj_data, codes, feature\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ_-0ri1h8UX",
        "outputId": "a93bbbdb-a38e-4e9b-f44d-e731a7782d08"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:25<00:00, 77.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Target price column to the data\n",
        "train_X.loc[:,'Target'] = train_y\n",
        "valid_X.loc[:,'Target'] = valid_y\n",
        "test_X.loc[:,'Target'] = test_y\n",
        "\n",
        "# Resetting the date index to numbers\n",
        "train_X.reset_index(inplace= True)\n",
        "valid_X.reset_index(inplace= True)\n",
        "test_X.reset_index(inplace= True)\n",
        "stock_price_adj_data.reset_index(inplace= True)\n",
        "\n",
        "#\n",
        "train_X = stock_price_adj_data[[\"Date\", \"SecuritiesCode\", \"Open\", \"Close\", \"Volume\"]].merge(train_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\n",
        "valid_X = stock_price_adj_data[[\"Date\", \"SecuritiesCode\", \"Open\", \"Close\", \"Volume\"]].merge(valid_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\n",
        "test_X = stock_price_adj_data[[\"Date\", \"SecuritiesCode\", \"Open\", \"Close\", \"Volume\"]].merge(test_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\n",
        "\n",
        "#\n",
        "train_X.dropna(inplace = True)\n",
        "valid_X.dropna(inplace = True)\n",
        "test_X.dropna(inplace = True)\n",
        "\n",
        "#\n",
        "train_y = train_X['Target']\n",
        "valid_y = valid_X['Target']\n",
        "test_y = test_X['Target']\n",
        "\n",
        "#\n",
        "train_X.drop(\"Target\", axis = 1, inplace = True)\n",
        "valid_X.drop(\"Target\", axis = 1, inplace = True)\n",
        "test_X.drop(\"Target\", axis = 1, inplace = True)\n",
        "\n",
        "#\n",
        "train_X.set_index('Date', inplace = True)\n",
        "valid_X.set_index('Date', inplace = True)\n",
        "test_X.set_index('Date', inplace = True)\n",
        "\n",
        "\n",
        "#\n",
        "old_test_X = test_X\n",
        "old_train_X = train_X\n",
        "old_valid_X = valid_X\n",
        "\n",
        "# Use below for manually selecting features\n",
        "# feat_cols = list(range(1, 10))\n",
        "# train_X = train_X.iloc[:, feat_cols]\n",
        "# valid_X = valid_X.iloc[:, feat_cols]\n",
        "# test_X = test_X.iloc[:, feat_cols]\n"
      ],
      "metadata": {
        "id": "xFLsBHQbh8ds"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train & valid split"
      ],
      "metadata": {
        "id": "mLjNSRkmkhlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X,y split\n",
        "X_train = train_X.values\n",
        "X_valid = valid_X.values\n",
        "X_test = test_X.values\n",
        "y_train = train_y.values\n",
        "y_test = test_y.values\n",
        "y_valid = valid_y.values\n",
        "print(\"train_X has shape\", X_train.shape)\n",
        "print(\"train_y has shape\", y_train.shape)\n",
        "print(\"valid_X has shape\", X_valid.shape)\n",
        "print(\"valid_y has shape\", y_valid.shape)\n",
        "print(\"test_X has shape\", X_test.shape)\n",
        "print(\"test_y has shape\", y_test.shape)"
      ],
      "metadata": {
        "id": "QW5e2Lm0R77J",
        "execution": {
          "iopub.status.busy": "2022-06-05T04:13:46.642019Z",
          "iopub.execute_input": "2022-06-05T04:13:46.642235Z",
          "iopub.status.idle": "2022-06-05T04:13:46.687233Z",
          "shell.execute_reply.started": "2022-06-05T04:13:46.64221Z",
          "shell.execute_reply": "2022-06-05T04:13:46.686454Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7254ec81-f227-4e7e-c213-7ff5f0bf1d4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X has shape (994642, 49)\n",
            "train_y has shape (994642,)\n",
            "valid_X has shape (393968, 49)\n",
            "valid_y has shape (393968,)\n",
            "test_X has shape (928606, 49)\n",
            "test_y has shape (928606,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.columns"
      ],
      "metadata": {
        "id": "yL-aHuKhBvXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c676d7-31fb-490b-e13a-15178490b602"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['SecuritiesCode', 'Open_x', 'Close_x', 'Volume', 'ExpectedDividend',\n",
              "       'High', 'Low', 'Open_y', 'Close_y', 'return_1day', 'Amplitude',\n",
              "       'OpentoClose', 'High52', 'MACD', 'return_5day', 'volatility_5day',\n",
              "       'MA_5day', 'EMA_5day', 'RSI_5day', 'MACD_5day', 'BIAS_5day',\n",
              "       'return_10day', 'volatility_10day', 'MA_10day', 'EMA_10day',\n",
              "       'RSI_10day', 'MACD_10day', 'BIAS_10day', 'return_20day',\n",
              "       'volatility_20day', 'MA_20day', 'EMA_20day', 'RSI_20day', 'MACD_20day',\n",
              "       'BIAS_20day', 'return_40day', 'volatility_40day', 'MA_40day',\n",
              "       'EMA_40day', 'RSI_40day', 'MACD_40day', 'BIAS_40day', 'return_60day',\n",
              "       'volatility_60day', 'MA_60day', 'EMA_60day', 'RSI_60day', 'MACD_60day',\n",
              "       'BIAS_60day'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "wxXkqwsik8X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCF:\n",
        "    SEED = 1\n",
        "    N_EPOCHS = 100\n",
        "    BATCH_SIZE = 2048\n",
        "    EARLY_STOPPING_PATIENCE = 10\n",
        "    EARLY_STOPPING_MIN_DELTA = 1e-5\n",
        "\n",
        "    # Transformer Parameters\n",
        "    EMBED_DIM=256//2\n",
        "    N_HEAD=8\n",
        "    FF_DIM=256//2\n",
        "    DROPOUT=0.001\n",
        "    N_BLOCK=4"
      ],
      "metadata": {
        "id": "dbO_Qjwt338k"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_dim = X_train.shape[-1]\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self,embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim=embed_dim\n",
        "        self.num_heads=num_heads\n",
        "        if embed_dim % num_heads !=0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim=embed_dim//num_heads\n",
        "        self.query_dense=layers.Dense(embed_dim)\n",
        "        self.key_dense=layers.Dense(embed_dim)\n",
        "        self.value_dense=layers.Dense(embed_dim)\n",
        "        self.combine_heads=layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self,query,key,value):\n",
        "        score=tf.matmul(query,key,transpose_b=True)\n",
        "        dim_key=tf.cast(tf.shape(key)[-1],tf.float32)\n",
        "        scaled_score=score/tf.math.sqrt(dim_key)\n",
        "        weights=tf.nn.softmax(scaled_score,axis=1)\n",
        "        output=tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self,inputs):\n",
        "        batch_size=tf.shape(inputs)[0]\n",
        "        query=self.query_dense(inputs)\n",
        "        key=self.key_dense(inputs)\n",
        "        value=self.value_dense(inputs)\n",
        "\n",
        "        query=self.separate_heads(\n",
        "            query,batch_size\n",
        "        )\n",
        "        key=self.separate_heads(\n",
        "            key,batch_size\n",
        "        )\n",
        "        value=self.separate_heads(\n",
        "            value,batch_size\n",
        "        )\n",
        "\n",
        "        attention,weights=self.attention(query,key,value)\n",
        "        attention=tf.transpose(\n",
        "            attention,perm=[0,2,1,3]\n",
        "        )\n",
        "        concat_attention=tf.reshape(\n",
        "            attention,(batch_size,-1,self.embed_dim)\n",
        "        )\n",
        "        output=self.combine_heads(\n",
        "            concat_attention\n",
        "        )\n",
        "        return output"
      ],
      "metadata": {
        "id": "KySlsFqXk9Cs",
        "execution": {
          "iopub.status.busy": "2022-06-05T04:13:46.688903Z",
          "iopub.execute_input": "2022-06-05T04:13:46.689142Z",
          "iopub.status.idle": "2022-06-05T04:13:46.704466Z",
          "shell.execute_reply.started": "2022-06-05T04:13:46.689114Z",
          "shell.execute_reply": "2022-06-05T04:13:46.703804Z"
        },
        "trusted": true
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self,embed_dim=GCF.EMBED_DIM,feat_dim=feat_dim,num_heads=GCF.N_HEAD,ff_dim=GCF.FF_DIM,rate=GCF.DROPOUT,**kwargs):\n",
        "        super(TransformerBlock,self).__init__()\n",
        "        self.att=MultiHeadSelfAttention(num_heads=num_heads,embed_dim=embed_dim)\n",
        "        self.ffn=keras.Sequential(\n",
        "            [layers.Dense(ff_dim,activation='gelu'),layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self,inputs,training):\n",
        "        attn_output=self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1= self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "c6N-3Jjfk9FD",
        "execution": {
          "iopub.status.busy": "2022-06-05T04:13:46.707553Z",
          "iopub.execute_input": "2022-06-05T04:13:46.707913Z",
          "iopub.status.idle": "2022-06-05T04:13:46.718386Z",
          "shell.execute_reply.started": "2022-06-05T04:13:46.707882Z",
          "shell.execute_reply": "2022-06-05T04:13:46.717415Z"
        },
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run"
      ],
      "metadata": {
        "id": "yz72vj4IlQnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    inputs=layers.Input(shape=(1,feat_dim))\n",
        "\n",
        "    x=layers.Dense(GCF.EMBED_DIM)(inputs)\n",
        "    x=layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    for k in range(GCF.N_BLOCK):\n",
        "        transformer_block=TransformerBlock(GCF.EMBED_DIM, feat_dim, GCF.N_HEAD, GCF.FF_DIM, GCF.DROPOUT)\n",
        "        x=transformer_block(x)\n",
        "\n",
        "    x=layers.GlobalAveragePooling1D()(x)\n",
        "    x=layers.Dense(20, activation=\"relu\")(x)\n",
        "\n",
        "    outputs=layers.Dense(1,activation='linear')(x)\n",
        "\n",
        "    model=keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "      optimizer=tf.optimizers.Adam(0.001, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9),\n",
        "      loss='mse',\n",
        "      metrics=[keras.metrics.RootMeanSquaredError()]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "create_model().summary()"
      ],
      "metadata": {
        "id": "2MEBXWaPyy1r",
        "execution": {
          "iopub.status.busy": "2022-06-05T04:13:46.724378Z",
          "iopub.execute_input": "2022-06-05T04:13:46.724586Z",
          "iopub.status.idle": "2022-06-05T04:13:48.300805Z",
          "shell.execute_reply.started": "2022-06-05T04:13:46.724561Z",
          "shell.execute_reply": "2022-06-05T04:13:48.299846Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c893007-2262-4d26-8f9c-df2d6bc4f8dc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1, 49)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1, 128)            6400      \n",
            "                                                                 \n",
            " layer_normalization (Layer  (None, 1, 128)            256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " transformer_block (Transfo  (None, None, 128)         99584     \n",
            " rmerBlock)                                                      \n",
            "                                                                 \n",
            " transformer_block_1 (Trans  (None, None, 128)         99584     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_2 (Trans  (None, None, 128)         99584     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " transformer_block_3 (Trans  (None, None, 128)         99584     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 128)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 20)                2580      \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407593 (1.55 MB)\n",
            "Trainable params: 407593 (1.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # model For GPU or TPU\n",
        "# with strategy.scope():\n",
        "#     model=create_model()\n",
        "\n",
        "# # model=create_model() # For CPU\n",
        "\n",
        "# early_stopping=keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=GCF.EARLY_STOPPING_PATIENCE,\n",
        "#     min_delta=GCF.EARLY_STOPPING_MIN_DELTA,\n",
        "#     restore_best_weights=True,\n",
        "# )\n",
        "\n",
        "# reduce_lr=ReduceLROnPlateau(\n",
        "#     monitor='val_loss',\n",
        "#     factor=0.5,\n",
        "#     patience=3,\n",
        "#     min_lr=1e-5,\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# #fit\n",
        "# history=model.fit(\n",
        "#     np.expand_dims(X_train,axis=1),y_train,\n",
        "#     validation_data=(np.expand_dims(X_valid,axis=1),y_valid),\n",
        "#     batch_size=GCF.BATCH_SIZE,\n",
        "#     epochs=GCF.N_EPOCHS,\n",
        "#     callbacks=[early_stopping,reduce_lr]\n",
        "# )\n",
        "\n",
        "# #predict\n",
        "# # valid_pred=model.predict(np.expand_dims(X_valid,axis=1))"
      ],
      "metadata": {
        "id": "vPsI-TIak9HZ",
        "execution": {
          "iopub.status.busy": "2022-06-05T04:13:48.302126Z",
          "iopub.execute_input": "2022-06-05T04:13:48.302383Z",
          "iopub.status.idle": "2022-06-05T04:15:55.850894Z",
          "shell.execute_reply.started": "2022-06-05T04:13:48.302354Z",
          "shell.execute_reply": "2022-06-05T04:15:55.849733Z"
        },
        "trusted": true
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# cols = [h.replace(\"val_\", \"\") for h in history.history.keys() if 'val' in h]\n",
        "\n",
        "# for c in cols:\n",
        "#     pd.DataFrame(history.history)[[c, \"val_\"+c]].plot() #plot\n",
        "#     plt.title(c)\n",
        "#     plt.show()\n",
        "\n",
        "# pd.DataFrame(history.history)['lr'].plot()\n",
        "# plt.title('lr')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "pEfAr5smk9JX",
        "execution": {
          "iopub.status.busy": "2022-06-05T04:15:55.852868Z",
          "iopub.execute_input": "2022-06-05T04:15:55.853999Z",
          "iopub.status.idle": "2022-06-05T04:15:56.580569Z",
          "shell.execute_reply.started": "2022-06-05T04:15:55.853948Z",
          "shell.execute_reply": "2022-06-05T04:15:56.579729Z"
        },
        "trusted": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Predicition on test\n",
        "# #predict\n",
        "# test_pred=model.predict(np.expand_dims(X_test,axis=1))"
      ],
      "metadata": {
        "id": "_IpURrThzEI-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sharpe Ratio"
      ],
      "metadata": {
        "id": "Vf0Hmi8LlUDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_rank(df):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        df (pd.DataFrame): including predict column\n",
        "    Returns:\n",
        "        df (pd.DataFrame): df with Rank\n",
        "    \"\"\"\n",
        "    # sort records to set Rank\n",
        "    df = df.sort_values(\"Predict\", ascending=False)\n",
        "    # set Rank starting from 0\n",
        "    df.loc[:, \"Rank\"] = np.arange(len(df[\"Predict\"]))\n",
        "    return df"
      ],
      "metadata": {
        "id": "hfQBsL0SzE6T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        df (pd.DataFrame): predicted results\n",
        "        portfolio_size (int): # of equities to buy/sell\n",
        "        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
        "    Returns:\n",
        "        (float): sharpe ratio\n",
        "    \"\"\"\n",
        "    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): predicted results\n",
        "            portfolio_size (int): # of equities to buy/sell\n",
        "            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
        "        Returns:\n",
        "            (float): spread return\n",
        "        \"\"\"\n",
        "        assert df['Rank'].min() == 0\n",
        "        assert df['Rank'].max() == len(df['Rank']) - 1\n",
        "        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
        "        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
        "        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
        "        return purchase - short\n",
        "\n",
        "    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n",
        "    sharpe_ratio = buf.mean() / buf.std()\n",
        "    return sharpe_ratio"
      ],
      "metadata": {
        "id": "8LXQLFwezFFE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "DOe7sgyb8hu8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result_Transformer = old_test_X[[\"SecuritiesCode\"]].copy()\n",
        "# result_Transformer.loc[:, \"Predict\"] = test_pred\n",
        "# result_Transformer.loc[:, 'Target'] = y_test\n",
        "\n",
        "# result_Transformer = result_Transformer.sort_values([\"Date\", \"Predict\"], ascending=[True, False])\n",
        "# result_Transformer = result_Transformer.groupby(\"Date\").apply(set_rank)\n",
        "# Transformer_test_rmse = sqrt(mean_squared_error(test_pred,y_test))"
      ],
      "metadata": {
        "id": "uWk6ISHlzFIc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Transformer RMSE\", Transformer_test_rmse)\n",
        "# print(\"Transformer Sharpe Ratio\", calc_spread_return_sharpe(result_Transformer, portfolio_size=200))"
      ],
      "metadata": {
        "id": "8DX-DAJK8ELJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "NLDvVEVVzFMH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WITH PCA"
      ],
      "metadata": {
        "id": "ZKdMIsxDCNye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data_key_features = feature.copy()\n",
        "data_codes = data_key_features[[\"SecuritiesCode\"]]\n",
        "\n",
        "pca = PCA(n_components = 'mle')\n",
        "data_components = pca.fit_transform(feature)\n",
        "\n",
        "data_components = pd.DataFrame(data_components)\n",
        "data_components[\"SecuritiesCode\"] = data_codes.values\n",
        "data_components[\"Date\"] = feature.index.values\n",
        "data_components.set_index(\"Date\", inplace=True)\n",
        "\n",
        "sum(pca.explained_variance_ratio_[:2]) # >95% of the variance\n",
        "\n",
        "data_components = data_components[[\"SecuritiesCode\", 0, 1, 2]] # first 3 components"
      ],
      "metadata": {
        "id": "8k2HvjsKCQli"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stock_price_data = pd.read_csv(\"/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n",
        "stock_price_adj_data = adjust_price(stock_price_data)\n",
        "\n",
        "# fetch prediction target SecuritiesCodes\n",
        "# There are 2000 codes\n",
        "codes = sorted(stock_price_adj_data[\"SecuritiesCode\"].unique())\n",
        "len(codes)\n",
        "\n",
        "# from tqdm import tqdm  # Smart progress meter\n",
        "# # generate the features for prediction\n",
        "# buff = []\n",
        "# for code in tqdm(codes):\n",
        "#     feat = get_features_for_predict(stock_price_adj_data, code)\n",
        "#     buff.append(feat)\n",
        "# feature = pd.concat(buff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09vz9-_6Sn-c",
        "outputId": "97e8a63f-1ca6-470e-cbcd-f269e72f374a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate feature/label\n",
        "train_X, train_y, valid_X, valid_y, test_X, test_y = get_features_and_label(\n",
        "    stock_price_adj_data, codes, data_components\n",
        ")"
      ],
      "metadata": {
        "id": "l7gC4AuU4Dp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c9d2f41-0049-4b6a-9b61-3c92e4876a95"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:25<00:00, 79.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Target price column to the data\n",
        "train_X.loc[:,'Target'] = train_y\n",
        "valid_X.loc[:,'Target'] = valid_y\n",
        "test_X.loc[:,'Target'] = test_y\n",
        "\n",
        "# Resetting the date index to numbers\n",
        "train_X.reset_index(inplace= True)\n",
        "valid_X.reset_index(inplace= True)\n",
        "test_X.reset_index(inplace= True)\n",
        "stock_price_adj_data.reset_index(inplace= True)\n",
        "\n",
        "#\n",
        "train_X = stock_price_adj_data[[\"Date\", \"SecuritiesCode\", \"Open\", \"Close\", \"Volume\"]].merge(train_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\n",
        "valid_X = stock_price_adj_data[[\"Date\", \"SecuritiesCode\", \"Open\", \"Close\", \"Volume\"]].merge(valid_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\n",
        "test_X = stock_price_adj_data[[\"Date\", \"SecuritiesCode\", \"Open\", \"Close\", \"Volume\"]].merge(test_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\n",
        "\n",
        "#\n",
        "train_X.dropna(inplace = True)\n",
        "valid_X.dropna(inplace = True)\n",
        "test_X.dropna(inplace = True)\n",
        "\n",
        "#\n",
        "train_y = train_X['Target']\n",
        "valid_y = valid_X['Target']\n",
        "test_y = test_X['Target']\n",
        "\n",
        "#\n",
        "train_X.drop(\"Target\", axis = 1, inplace = True)\n",
        "valid_X.drop(\"Target\", axis = 1, inplace = True)\n",
        "test_X.drop(\"Target\", axis = 1, inplace = True)\n",
        "\n",
        "#\n",
        "train_X.set_index('Date', inplace = True)\n",
        "valid_X.set_index('Date', inplace = True)\n",
        "test_X.set_index('Date', inplace = True)\n",
        "\n",
        "\n",
        "#\n",
        "old_test_X = test_X\n",
        "old_train_X = train_X\n",
        "old_valid_X = valid_X\n",
        "\n",
        "# Use below for manually selecting features\n",
        "# feat_cols = list(range(1, 10))\n",
        "# train_X = train_X.iloc[:, feat_cols]\n",
        "# valid_X = valid_X.iloc[:, feat_cols]\n",
        "# test_X = test_X.iloc[:, feat_cols]\n",
        "\n",
        "feat_cols = list(range(1, 4))\n",
        "train_X = train_X.iloc[:, feat_cols]\n",
        "valid_X = valid_X.iloc[:, feat_cols]\n",
        "test_X = test_X.iloc[:, feat_cols]\n"
      ],
      "metadata": {
        "id": "00vQLaRdF5kK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X,y split\n",
        "X_train = train_X.values\n",
        "X_valid = valid_X.values\n",
        "X_test = test_X.values\n",
        "y_train = train_y.values\n",
        "y_test = test_y.values\n",
        "y_valid = valid_y.values\n",
        "print(\"train_X has shape\", X_train.shape)\n",
        "print(\"train_y has shape\", y_train.shape)\n",
        "print(\"valid_X has shape\", X_valid.shape)\n",
        "print(\"valid_y has shape\", y_valid.shape)\n",
        "print(\"test_X has shape\", X_test.shape)\n",
        "print(\"test_y has shape\", y_test.shape)"
      ],
      "metadata": {
        "id": "KSNPnX5yG9qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3fd6f44-72b7-421d-e32b-9b8c0b1b04fd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X has shape (994642, 3)\n",
            "train_y has shape (994642,)\n",
            "valid_X has shape (393968, 3)\n",
            "valid_y has shape (393968,)\n",
            "test_X has shape (928606, 3)\n",
            "test_y has shape (928606,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_subsets = {\n",
        "    \"012\": {\"X_train\": X_train, \"X_test\": X_test, \"X_valid\": X_valid},\n",
        "    \"01\": {\"X_train\": X_train[:, [0, 1]], \"X_valid\": X_valid[:, [0, 1]], \"X_test\": X_test[:, [0, 1]]},\n",
        "    \"02\": {\"X_train\": X_train[:, [0, 2]], \"X_valid\": X_valid[:, [0, 2]], \"X_test\": X_test[:, [0, 2]]},\n",
        "    \"12\": {\"X_train\": X_train[:, [1, 2]], \"X_valid\": X_valid[:, [1, 2]], \"X_test\": X_test[:, [1, 2]]},\n",
        "    \"0\": {\"X_train\": X_train[:, [0]], \"X_valid\": X_valid[:, [0]], \"X_test\": X_test[:, [0]]},\n",
        "    \"1\": {\"X_train\": X_train[:, [1]], \"X_valid\": X_valid[:, [1]], \"X_test\": X_test[:, [1]]},\n",
        "    \"2\": {\"X_train\": X_train[:, [2]], \"X_valid\": X_valid[:, [2]], \"X_test\": X_test[:, [2]]}\n",
        "}\n",
        "\n",
        "feature_subsets_list = [\"012\", \"01\", \"02\", \"12\", \"0\", \"1\", \"2\"]"
      ],
      "metadata": {
        "id": "mvdaNsyuHS5D"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SR_Transformer = dict()\n",
        "RMSE_Transformer = dict()"
      ],
      "metadata": {
        "id": "SGVZGvdjHULe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subset_key in tqdm(feature_subsets_list):\n",
        "    X_train = np.reshape(feature_subsets[subset_key]['X_train'],\n",
        "                         (feature_subsets[subset_key]['X_train'].shape[0],\n",
        "                          feature_subsets[subset_key]['X_train'].shape[1]))\n",
        "    X_test = np.reshape(feature_subsets[subset_key]['X_test'],\n",
        "                         (feature_subsets[subset_key]['X_test'].shape[0],\n",
        "                          feature_subsets[subset_key]['X_test'].shape[1]))\n",
        "    X_valid = np.reshape(feature_subsets[subset_key]['X_valid'],\n",
        "                         (feature_subsets[subset_key]['X_valid'].shape[0],\n",
        "                          feature_subsets[subset_key]['X_valid'].shape[1]))\n",
        "    feat_dim = X_train.shape[-1]\n",
        "\n",
        "    class MultiHeadSelfAttention(layers.Layer):\n",
        "      def __init__(self,embed_dim, num_heads=8):\n",
        "          super(MultiHeadSelfAttention, self).__init__()\n",
        "          self.embed_dim=embed_dim\n",
        "          self.num_heads=num_heads\n",
        "          if embed_dim % num_heads !=0:\n",
        "              raise ValueError(\n",
        "                  f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "              )\n",
        "          self.projection_dim=embed_dim//num_heads\n",
        "          self.query_dense=layers.Dense(embed_dim)\n",
        "          self.key_dense=layers.Dense(embed_dim)\n",
        "          self.value_dense=layers.Dense(embed_dim)\n",
        "          self.combine_heads=layers.Dense(embed_dim)\n",
        "\n",
        "      def attention(self,query,key,value):\n",
        "          score=tf.matmul(query,key,transpose_b=True)\n",
        "          dim_key=tf.cast(tf.shape(key)[-1],tf.float32)\n",
        "          scaled_score=score/tf.math.sqrt(dim_key)\n",
        "          weights=tf.nn.softmax(scaled_score,axis=1)\n",
        "          output=tf.matmul(weights, value)\n",
        "          return output, weights\n",
        "\n",
        "      def separate_heads(self, x, batch_size):\n",
        "          x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "          return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "      def call(self,inputs):\n",
        "          batch_size=tf.shape(inputs)[0]\n",
        "          query=self.query_dense(inputs)\n",
        "          key=self.key_dense(inputs)\n",
        "          value=self.value_dense(inputs)\n",
        "\n",
        "          query=self.separate_heads(\n",
        "              query,batch_size\n",
        "          )\n",
        "          key=self.separate_heads(\n",
        "              key,batch_size\n",
        "          )\n",
        "          value=self.separate_heads(\n",
        "              value,batch_size\n",
        "          )\n",
        "\n",
        "          attention,weights=self.attention(query,key,value)\n",
        "          attention=tf.transpose(\n",
        "              attention,perm=[0,2,1,3]\n",
        "          )\n",
        "          concat_attention=tf.reshape(\n",
        "              attention,(batch_size,-1,self.embed_dim)\n",
        "          )\n",
        "          output=self.combine_heads(\n",
        "              concat_attention\n",
        "          )\n",
        "          return output\n",
        "\n",
        "    class TransformerBlock(layers.Layer):\n",
        "      def __init__(self,embed_dim=GCF.EMBED_DIM,feat_dim=feat_dim,num_heads=GCF.N_HEAD,ff_dim=GCF.FF_DIM,rate=GCF.DROPOUT,**kwargs):\n",
        "          super(TransformerBlock,self).__init__()\n",
        "          self.att=MultiHeadSelfAttention(num_heads=num_heads,embed_dim=embed_dim)\n",
        "          self.ffn=keras.Sequential(\n",
        "              [layers.Dense(ff_dim,activation='gelu'),layers.Dense(embed_dim),]\n",
        "          )\n",
        "          self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "          self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "          self.dropout1 = layers.Dropout(rate)\n",
        "          self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "      def call(self,inputs,training):\n",
        "          attn_output=self.att(inputs)\n",
        "          attn_output = self.dropout1(attn_output, training=training)\n",
        "          out1= self.layernorm1(inputs + attn_output)\n",
        "          ffn_output = self.ffn(out1)\n",
        "          ffn_output = self.dropout2(ffn_output, training=training)\n",
        "          return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "      def create_model():\n",
        "          inputs=layers.Input(shape=(1,feat_dim))\n",
        "\n",
        "          x=layers.Dense(GCF.EMBED_DIM)(inputs)\n",
        "          x=layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "          for k in range(GCF.N_BLOCK):\n",
        "              transformer_block=TransformerBlock(GCF.EMBED_DIM, feat_dim, GCF.N_HEAD, GCF.FF_DIM, GCF.DROPOUT)\n",
        "              x=transformer_block(x)\n",
        "\n",
        "          x=layers.GlobalAveragePooling1D()(x)\n",
        "          x=layers.Dense(20, activation=\"relu\")(x)\n",
        "\n",
        "          outputs=layers.Dense(1,activation='linear')(x)\n",
        "\n",
        "          model=keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "          model.compile(\n",
        "            optimizer=tf.optimizers.Adam(0.001, beta_1=0.9, beta_2=0.98,\n",
        "                                          epsilon=1e-9),\n",
        "            loss='mse',\n",
        "            metrics=[keras.metrics.RootMeanSquaredError()]\n",
        "          )\n",
        "          return model\n",
        "\n",
        "\n",
        "      # model For GPU or TPU\n",
        "      with strategy.scope():\n",
        "          model=create_model()\n",
        "\n",
        "      # model=create_model() # For CPU\n",
        "\n",
        "      early_stopping=keras.callbacks.EarlyStopping(\n",
        "          monitor='val_loss',\n",
        "          patience=GCF.EARLY_STOPPING_PATIENCE,\n",
        "          min_delta=GCF.EARLY_STOPPING_MIN_DELTA,\n",
        "          restore_best_weights=True,\n",
        "      )\n",
        "\n",
        "      reduce_lr=ReduceLROnPlateau(\n",
        "          monitor='val_loss',\n",
        "          factor=0.5,\n",
        "          patience=3,\n",
        "          min_lr=1e-5,\n",
        "          verbose=1\n",
        "      )\n",
        "\n",
        "      #fit\n",
        "      history=model.fit(\n",
        "          np.expand_dims(X_train,axis=1),y_train,\n",
        "          validation_data=(np.expand_dims(X_valid,axis=1),y_valid),\n",
        "          batch_size=GCF.BATCH_SIZE,\n",
        "          epochs=GCF.N_EPOCHS,\n",
        "          callbacks=[early_stopping,reduce_lr]\n",
        "      )\n",
        "\n",
        "    #predict\n",
        "      test_pred=model.predict(np.expand_dims(X_test,axis=1))\n",
        "    # test_pred=model.predict(np.expand_dims(X_test,axis=1))\n",
        "\n",
        "\n",
        "      result_Transformer = old_test_X[[\"SecuritiesCode\"]].copy()\n",
        "      result_Transformer.loc[:, \"Predict\"] = test_pred\n",
        "      result_Transformer.loc[:, 'Target'] = y_test\n",
        "      result_Transformer = result_Transformer.sort_values([\"Date\", \"Predict\"], ascending=[True, False])\n",
        "      result_Transformer = result_Transformer.groupby(\"Date\").apply(set_rank)\n",
        "      Transformer_test_rmse = sqrt(mean_squared_error(test_pred,y_test))\n",
        "\n",
        "      SR_Transformer[subset_key] = calc_spread_return_sharpe(result_Transformer, portfolio_size=200)\n",
        "      RMSE_Transformer[subset_key] = Transformer_test_rmse"
      ],
      "metadata": {
        "id": "H3rq5Yz8HUO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73fa8b3b-938e-4afa-f1a9-ed33187d778a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "486/486 [==============================] - 41s 45ms/step - loss: 0.0278 - root_mean_squared_error: 0.1669 - val_loss: 0.0026 - val_root_mean_squared_error: 0.0509 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 0.0011 - root_mean_squared_error: 0.0325 - val_loss: 5.7839e-04 - val_root_mean_squared_error: 0.0240 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 7.0313e-04 - root_mean_squared_error: 0.0265 - val_loss: 4.3296e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 5.7457e-04 - root_mean_squared_error: 0.0240 - val_loss: 4.3508e-04 - val_root_mean_squared_error: 0.0209 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 5.3271e-04 - root_mean_squared_error: 0.0231 - val_loss: 5.5945e-04 - val_root_mean_squared_error: 0.0237 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "484/486 [============================>.] - ETA: 0s - loss: 5.1325e-04 - root_mean_squared_error: 0.0227\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 5.1331e-04 - root_mean_squared_error: 0.0227 - val_loss: 5.0370e-04 - val_root_mean_squared_error: 0.0224 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.9345e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.4791e-04 - val_root_mean_squared_error: 0.0212 - lr: 5.0000e-04\n",
            "Epoch 8/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.9383e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.3523e-04 - val_root_mean_squared_error: 0.0209 - lr: 5.0000e-04\n",
            "Epoch 9/100\n",
            "484/486 [============================>.] - ETA: 0s - loss: 4.9537e-04 - root_mean_squared_error: 0.0223\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.9542e-04 - root_mean_squared_error: 0.0223 - val_loss: 4.6653e-04 - val_root_mean_squared_error: 0.0216 - lr: 5.0000e-04\n",
            "Epoch 10/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.9307e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.3402e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 11/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8628e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.4071e-04 - val_root_mean_squared_error: 0.0210 - lr: 2.5000e-04\n",
            "Epoch 12/100\n",
            "484/486 [============================>.] - ETA: 0s - loss: 4.8764e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.8765e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3200e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 13/100\n",
            "486/486 [==============================] - 12s 26ms/step - loss: 4.8571e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3281e-04 - val_root_mean_squared_error: 0.0208 - lr: 1.2500e-04\n",
            "29019/29019 [==============================] - 275s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 1/7 [07:51<47:06, 471.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "486/486 [==============================] - 41s 46ms/step - loss: 0.0343 - root_mean_squared_error: 0.1853 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0344 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "486/486 [==============================] - 12s 25ms/step - loss: 9.6783e-04 - root_mean_squared_error: 0.0311 - val_loss: 5.5906e-04 - val_root_mean_squared_error: 0.0236 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "486/486 [==============================] - 12s 25ms/step - loss: 6.1403e-04 - root_mean_squared_error: 0.0248 - val_loss: 4.3213e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.4582e-04 - root_mean_squared_error: 0.0234 - val_loss: 4.3500e-04 - val_root_mean_squared_error: 0.0209 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.1471e-04 - root_mean_squared_error: 0.0227 - val_loss: 4.5034e-04 - val_root_mean_squared_error: 0.0212 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 5.0558e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.0558e-04 - root_mean_squared_error: 0.0225 - val_loss: 4.6455e-04 - val_root_mean_squared_error: 0.0216 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.9164e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.5263e-04 - val_root_mean_squared_error: 0.0213 - lr: 5.0000e-04\n",
            "Epoch 8/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.9038e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3391e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 9/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 4.9232e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.9232e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.3335e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 10/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8725e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3178e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 11/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.8739e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3175e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 12/100\n",
            "485/486 [============================>.] - ETA: 0s - loss: 4.8698e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.8699e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3264e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 13/100\n",
            "486/486 [==============================] - 13s 26ms/step - loss: 4.8550e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3236e-04 - val_root_mean_squared_error: 0.0208 - lr: 1.2500e-04\n",
            "29019/29019 [==============================] - 267s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 2/7 [15:34<38:53, 466.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "486/486 [==============================] - 41s 46ms/step - loss: 0.0135 - root_mean_squared_error: 0.1160 - val_loss: 6.0518e-04 - val_root_mean_squared_error: 0.0246 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 6.0027e-04 - root_mean_squared_error: 0.0245 - val_loss: 4.3594e-04 - val_root_mean_squared_error: 0.0209 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.1072e-04 - root_mean_squared_error: 0.0226 - val_loss: 4.4102e-04 - val_root_mean_squared_error: 0.0210 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.9469e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.4319e-04 - val_root_mean_squared_error: 0.0211 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 4.8733e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.8733e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3181e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.8308e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3181e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 7/100\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.8301e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3180e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 8/100\n",
            "485/486 [============================>.] - ETA: 0s - loss: 4.8300e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8302e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3175e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 9/100\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.8291e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3176e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 10/100\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.8291e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3173e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 11/100\n",
            "484/486 [============================>.] - ETA: 0s - loss: 4.8282e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8287e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3183e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 12/100\n",
            "486/486 [==============================] - 12s 26ms/step - loss: 4.8284e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3173e-04 - val_root_mean_squared_error: 0.0208 - lr: 1.2500e-04\n",
            "29019/29019 [==============================] - 270s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 3/7 [23:07<30:40, 460.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "486/486 [==============================] - 41s 46ms/step - loss: 0.0169 - root_mean_squared_error: 0.1301 - val_loss: 0.0021 - val_root_mean_squared_error: 0.0454 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "486/486 [==============================] - 12s 25ms/step - loss: 6.9977e-04 - root_mean_squared_error: 0.0265 - val_loss: 4.3243e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "486/486 [==============================] - 12s 25ms/step - loss: 5.5934e-04 - root_mean_squared_error: 0.0237 - val_loss: 4.3179e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.0059e-04 - root_mean_squared_error: 0.0224 - val_loss: 4.3181e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 4.8302e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.8302e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3173e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8506e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3612e-04 - val_root_mean_squared_error: 0.0209 - lr: 5.0000e-04\n",
            "Epoch 7/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8298e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3197e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 8/100\n",
            "484/486 [============================>.] - ETA: 0s - loss: 4.8269e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8291e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3176e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 9/100\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.8286e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3180e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 10/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8287e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3191e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 11/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 4.8288e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8288e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3188e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 12/100\n",
            "486/486 [==============================] - 12s 25ms/step - loss: 4.8283e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3178e-04 - val_root_mean_squared_error: 0.0208 - lr: 1.2500e-04\n",
            "29019/29019 [==============================] - 270s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 4/7 [30:40<22:51, 457.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "486/486 [==============================] - 41s 46ms/step - loss: 0.0271 - root_mean_squared_error: 0.1646 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0338 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 6.4821e-04 - root_mean_squared_error: 0.0255 - val_loss: 4.7422e-04 - val_root_mean_squared_error: 0.0218 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.3343e-04 - root_mean_squared_error: 0.0231 - val_loss: 4.3292e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.0220e-04 - root_mean_squared_error: 0.0224 - val_loss: 4.3312e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "485/486 [============================>.] - ETA: 0s - loss: 4.9268e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.9271e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.3772e-04 - val_root_mean_squared_error: 0.0209 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.8714e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3249e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 7/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.8595e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3360e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 8/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 4.8541e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.8541e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3288e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 9/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8463e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3193e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 10/100\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.8387e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3281e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 11/100\n",
            "485/486 [============================>.] - ETA: 0s - loss: 4.8433e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.8427e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3331e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 12/100\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 4.8368e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3222e-04 - val_root_mean_squared_error: 0.0208 - lr: 1.2500e-04\n",
            "Epoch 13/100\n",
            "486/486 [==============================] - 13s 26ms/step - loss: 4.8358e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3179e-04 - val_root_mean_squared_error: 0.0208 - lr: 1.2500e-04\n",
            "29019/29019 [==============================] - 269s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████▏  | 5/7 [38:24<15:19, 459.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "486/486 [==============================] - 42s 47ms/step - loss: 0.0309 - root_mean_squared_error: 0.1756 - val_loss: 6.7392e-04 - val_root_mean_squared_error: 0.0260 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 8.9697e-04 - root_mean_squared_error: 0.0299 - val_loss: 5.3951e-04 - val_root_mean_squared_error: 0.0232 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "486/486 [==============================] - 12s 25ms/step - loss: 6.2606e-04 - root_mean_squared_error: 0.0250 - val_loss: 4.3182e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.4507e-04 - root_mean_squared_error: 0.0233 - val_loss: 4.6960e-04 - val_root_mean_squared_error: 0.0217 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.2147e-04 - root_mean_squared_error: 0.0228 - val_loss: 4.4687e-04 - val_root_mean_squared_error: 0.0211 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "485/486 [============================>.] - ETA: 0s - loss: 5.0770e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "486/486 [==============================] - 11s 24ms/step - loss: 5.0769e-04 - root_mean_squared_error: 0.0225 - val_loss: 4.4532e-04 - val_root_mean_squared_error: 0.0211 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8946e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.5129e-04 - val_root_mean_squared_error: 0.0212 - lr: 5.0000e-04\n",
            "Epoch 8/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8838e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3175e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 9/100\n",
            "484/486 [============================>.] - ETA: 0s - loss: 4.8742e-04 - root_mean_squared_error: 0.0221\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8734e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3454e-04 - val_root_mean_squared_error: 0.0208 - lr: 5.0000e-04\n",
            "Epoch 10/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8659e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3180e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 11/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8562e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3418e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 12/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 4.8449e-04 - root_mean_squared_error: 0.0220\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.8449e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3176e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 13/100\n",
            "486/486 [==============================] - 12s 25ms/step - loss: 4.8393e-04 - root_mean_squared_error: 0.0220 - val_loss: 4.3178e-04 - val_root_mean_squared_error: 0.0208 - lr: 1.2500e-04\n",
            "29019/29019 [==============================] - 268s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 6/7 [46:06<07:40, 460.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "486/486 [==============================] - 43s 46ms/step - loss: 0.0736 - root_mean_squared_error: 0.2713 - val_loss: 4.3455e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 0.0020 - root_mean_squared_error: 0.0446 - val_loss: 0.0017 - val_root_mean_squared_error: 0.0413 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 8.3377e-04 - root_mean_squared_error: 0.0289 - val_loss: 4.3173e-04 - val_root_mean_squared_error: 0.0208 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 7.3145e-04 - root_mean_squared_error: 0.0270\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 7.3145e-04 - root_mean_squared_error: 0.0270 - val_loss: 4.9605e-04 - val_root_mean_squared_error: 0.0223 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.3838e-04 - root_mean_squared_error: 0.0232 - val_loss: 5.5844e-04 - val_root_mean_squared_error: 0.0236 - lr: 5.0000e-04\n",
            "Epoch 6/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.2252e-04 - root_mean_squared_error: 0.0229 - val_loss: 4.8078e-04 - val_root_mean_squared_error: 0.0219 - lr: 5.0000e-04\n",
            "Epoch 7/100\n",
            "486/486 [==============================] - ETA: 0s - loss: 5.0835e-04 - root_mean_squared_error: 0.0225\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 5.0835e-04 - root_mean_squared_error: 0.0225 - val_loss: 4.4032e-04 - val_root_mean_squared_error: 0.0210 - lr: 5.0000e-04\n",
            "Epoch 8/100\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.9259e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.3643e-04 - val_root_mean_squared_error: 0.0209 - lr: 2.5000e-04\n",
            "Epoch 9/100\n",
            "486/486 [==============================] - 12s 24ms/step - loss: 4.9252e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.3372e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 10/100\n",
            "485/486 [============================>.] - ETA: 0s - loss: 4.9293e-04 - root_mean_squared_error: 0.0222\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "486/486 [==============================] - 11s 23ms/step - loss: 4.9283e-04 - root_mean_squared_error: 0.0222 - val_loss: 4.3444e-04 - val_root_mean_squared_error: 0.0208 - lr: 2.5000e-04\n",
            "Epoch 11/100\n",
            "486/486 [==============================] - 12s 25ms/step - loss: 4.8830e-04 - root_mean_squared_error: 0.0221 - val_loss: 4.3760e-04 - val_root_mean_squared_error: 0.0209 - lr: 1.2500e-04\n",
            "29019/29019 [==============================] - 272s 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [53:31<00:00, 458.74s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # model For GPU or TPU\n",
        "# with strategy.scope():\n",
        "#     model=create_model()\n",
        "\n",
        "# # model=create_model() # For CPU\n",
        "\n",
        "# early_stopping=keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=GCF.EARLY_STOPPING_PATIENCE,\n",
        "#     min_delta=GCF.EARLY_STOPPING_MIN_DELTA,\n",
        "#     restore_best_weights=True,\n",
        "# )\n",
        "\n",
        "# reduce_lr=ReduceLROnPlateau(\n",
        "#     monitor='val_loss',\n",
        "#     factor=0.5,\n",
        "#     patience=3,\n",
        "#     min_lr=1e-5,\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# #fit\n",
        "# history=model.fit(\n",
        "#     np.expand_dims(X_train,axis=1),y_train,\n",
        "#     validation_data=(np.expand_dims(X_valid,axis=1),y_valid),\n",
        "#     batch_size=GCF.BATCH_SIZE,\n",
        "#     epochs=GCF.N_EPOCHS,\n",
        "#     callbacks=[early_stopping,reduce_lr]\n",
        "# )\n",
        "\n",
        "# #predict\n",
        "# valid_pred=model.predict(np.expand_dims(X_valid,axis=1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-05T04:13:48.302126Z",
          "iopub.execute_input": "2022-06-05T04:13:48.302383Z",
          "iopub.status.idle": "2022-06-05T04:15:55.850894Z",
          "shell.execute_reply.started": "2022-06-05T04:13:48.302354Z",
          "shell.execute_reply": "2022-06-05T04:15:55.849733Z"
        },
        "trusted": true,
        "id": "oxr2E4i6hf9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SR_Transformer"
      ],
      "metadata": {
        "id": "XuXv5N_3HUTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee976703-7ad6-46ea-ae28-9bc57d000d77"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'012': 0.007407379967842348,\n",
              " '01': 0.08973948941257964,\n",
              " '02': 0.0019416877414624373,\n",
              " '12': -0.01569491422462193,\n",
              " '0': 0.024679971081948023,\n",
              " '1': 0.027527421754791913,\n",
              " '2': 0.013697597177703066}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RMSE_Transformer"
      ],
      "metadata": {
        "id": "DSkluX2JHUWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e8a336-ddfb-4357-b453-84fa6467250b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'012': 0.02583693035302078,\n",
              " '01': 0.025831599081047884,\n",
              " '02': 0.025913746311948562,\n",
              " '12': 0.025827745096109928,\n",
              " '0': 0.025833658552258495,\n",
              " '1': 0.025818277014256676,\n",
              " '2': 0.02588364796169937}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xuBsVCjoHUYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4cI0_t6mHU2T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}